{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy NNets for education pourpouses in  Julia\n",
    "\n",
    "Interesting discussion how to make forward pass efficiently using BLAS:\n",
    "\n",
    "- https://discourse.julialang.org/t/blas-performance-issues-for-common-neural-network-patterns/565\n",
    "\n",
    "- http://int8.io/neural-networks-in-julia-hyperbolic-tangent-and-relu/\n",
    "\n",
    "- http://int8.io/backpropagation-from-scratch-in-julia-part-ii-derivation-and-implementation/\n",
    "\n",
    "\n",
    "#### Add the follwoing packages before executing this notebok\n",
    "\n",
    "- Pkg.add(\"MNIST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[5.0,0.0,4.0,1.0,9.0,2.0,1.0,3.0,1.0,4.0  …  9.0,2.0,9.0,5.0,1.0,8.0,3.0,5.0,6.0,8.0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = MNIST.traindata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train[1];\n",
    "y_train = Vector{Int32}(train[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int32,1}:\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort(unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Linear layer and relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "n_visible = 784\n",
    "n_hidden = 500\n",
    "\n",
    "srand(1234)\n",
    "W1 = randn(T, n_hidden, n_visible );\n",
    "W1 = W1/norm(W1)\n",
    "b = zeros(n_hidden);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(X_train[:,1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#linear layer\n",
    "batch = W1 * X_train[:,1:3].+b\n",
    "\n",
    "#relu\n",
    "l1_batch = batch .* (batch .>0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp(batch)./sum(exp(batch),1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 3.69189e74  3.80324e69  4.13143e48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(exp(l1_batch),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp(l1_batch)./sum(exp(l1_batch),1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type LinearLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    b::Vector{T}\n",
    "    seed::Int\n",
    "    \n",
    "    function LinearLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,\n",
    "                   output,\n",
    "                   randn(T,output,input)/sqrt(input),\n",
    "                   zeros(output))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearLayer{Float32}(784,500,Float32[0.0309767 0.0511305 … 0.0373952 -0.00012965; -0.0322051 -0.0588398 … 0.0261659 0.0156726; … ; -0.0580577 0.0256086 … 0.116446 -0.0452519; -0.0132269 -0.00453994 … 0.0292845 0.0139892],Float32[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "output_dim = 500\n",
    "l = LinearLayer{Float32}(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type ReluActivation{T}\n",
    "    \"\"\"\n",
    "    Relu Activation function latyer\n",
    "    \"\"\"\n",
    "    dim::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type SoftMaxLayer{T}\n",
    "    \"\"\"\n",
    "    Standard layer between activations.\n",
    "    The output of this layer for a given input is meant to be a matrix product \n",
    "    of the input times W\n",
    "    \"\"\"\n",
    "    input_dim::Int\n",
    "    output_dim::Int\n",
    "    W::Array{T}\n",
    "    seed::Int\n",
    "    \n",
    "    function SoftMaxLayer(input, output; seed=1234)\n",
    "        srand(seed)\n",
    "        return new(input,\n",
    "                   output,\n",
    "                   randn(T,output, input)/sqrt(input))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "About Softmax layer\n",
    "\n",
    "http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example\n",
    "\n",
    "Now let us define the structure, weight types (float type) of a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 500\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim , hidden_dim),\n",
    "                 ReluActivation{Float32}(hidden_dim),\n",
    "                 SoftMaxLayer{Float32}(hidden_dim, output_dim)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the code that accepts something like this\n",
    "# mlp(784,500,10, [\"sigmoid\", \"softmax\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making predictions with the network\n",
    "\n",
    "We have defined a MLP as list of layers and activation functions.\n",
    "\n",
    "In order to make a prediction we need to make a forward pass through the network.\n",
    "Let us assume by now that we have a good set of weights at each layer in the network and\n",
    "we want to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sparse vector of length 8 with 2 Int64 nonzero entries:\n",
       "  [6]  =  1\n",
       "  [7]  =  1\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse([0,0,0,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 3 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(linear_layer::LinearLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Given an input batch where the data comes as columns this method propagates \n",
    "    the batch using the weights of the linear layer\n",
    "    \"\"\"\n",
    "    return linear_layer.W * Xbatch .+ linear_layer.b\n",
    "end\n",
    "\n",
    "function forward(relu_activation::ReluActivation, Xbatch::Array)\n",
    "    return Xbatch.*( Xbatch .> 0.)\n",
    "end\n",
    "\n",
    "function forward(softmax_layer::SoftMaxLayer, Xbatch::Array)\n",
    "    \"\"\"\n",
    "    Layer shrinking the output to [0,1] values.\n",
    "    Notice that sum(exp(Xbatch),1) will generate a Matrix with as many elements as\n",
    "    columns in Xbatch. \n",
    "    \"\"\"\n",
    "    Xbatch_out = softmax_layer.W * Xbatch\n",
    "    return exp(Xbatch_out)./sum(exp(Xbatch_out), 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xbatch = X_train[:,1:25];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,25)(500,25)(10,25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[2], aux)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[3], aux)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_proba (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict_proba(mlp, Xbatch::Array)\n",
    "    for l in mlp\n",
    "        Xbatch = forward(l, Xbatch)\n",
    "    end\n",
    "    return Xbatch\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.540603 seconds (745.08 k allocations: 27.207 MB, 1.12% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×25 Array{Float64,2}:\n",
       " 5.39905e-22  1.0           9.17328e-30  …  9.1359e-32   8.48121e-20\n",
       " 2.37617e-5   1.11199e-49   2.55392e-68     5.00068e-19  2.31968e-43\n",
       " 6.93362e-51  1.8805e-97    5.01312e-33     9.6184e-80   1.53352e-83\n",
       " 3.41924e-13  1.70124e-72   8.44621e-12     1.31159e-34  4.3633e-48 \n",
       " 2.1359e-42   2.54938e-76   1.57863e-45     5.86561e-11  9.00681e-47\n",
       " 7.27582e-16  5.33328e-55   1.75377e-23  …  4.57536e-48  1.0        \n",
       " 0.000291457  1.23333e-74   1.0             1.0          1.73416e-63\n",
       " 8.83703e-17  1.10494e-125  1.52251e-36     1.17959e-45  3.82412e-82\n",
       " 1.68701e-43  4.17899e-67   3.65337e-25     2.82104e-50  3.34436e-60\n",
       " 0.999685     6.36062e-51   7.87676e-32     2.7335e-64   1.85022e-33"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each column contains a vector that represents\n",
    "# The conditional probability of the target beein from a particular class having observed\n",
    "# the input vector.\n",
    "\n",
    "@time predict_proba(mlp_classifer, Xbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Encoding class values as \"onehot\" vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one_hot_encoding (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function one_hot_encoding(y_train::Vector,\n",
    "                          unique_classes::Vector,\n",
    "                          class_to_pos::Dict)\n",
    "    \n",
    "    encoded_classes = zeros(length(unique_classes), length(y_train))\n",
    "    for (i,y) in enumerate(y_train)\n",
    "        encoded_classes[class_to_pos[y],i] = 1\n",
    "    end\n",
    "    return encoded_classes\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_classes = sort(unique(y_train))\n",
    "class_to_pos = Dict(class=>pos for (pos,class) in enumerate(unique_classes));    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class integer: Int32[5,0,4]\n",
      "Encoding:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10×3 Array{Float64,2}:\n",
       " 0.0  1.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  1.0\n",
       " 1.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nclass integer: \", y_train[1:3])\n",
    "print(\"\\nEncoding:\\n\")\n",
    "one_hot_encoding(y_train[1:3], unique_classes, class_to_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "\n",
    "## Cross entropy loss for mlp_classifier\n",
    "\n",
    "We will focus now on an standard loss function for classification problems. The cross entropy loss.\n",
    "\n",
    "$$\n",
    "    \\text{loss}\\left(h(x), e(t) \\right) = \\sum_{i=1}^{C} e(t)_i \\log(h(x)_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type crossentropy_loss\n",
    "    dim::Int \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossentropy_loss(10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = crossentropy_loss(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 4 methods)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forward(loss::crossentropy_loss, Y_hat_batch::Matrix, Y_encoded::Matrix)\n",
    "    \"\"\"\n",
    "    Should this function do the onehot encoding?\n",
    "    In order to save memory it seems reasonable but...\n",
    "    \n",
    "    Returns the loss between the batch\n",
    "    \"\"\"\n",
    "    return sum(Y_encoded.*log(Y_hat_batch))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradients \n",
    "\n",
    "## Le us compute the gradient of the loss for a given input vector\n",
    "\n",
    "Now we will deal with the learning part. That is, given a MLP architecture we will tune the weights in order to minimize some error function. \n",
    "\n",
    "- Let $z^L$ be the preactivation at layer $L$.\n",
    "- Let $h(x)$ be the output values of the network.\n",
    "- Let $e(y)$ be the encoding of class $y$.\n",
    "\n",
    "\n",
    "### Equation for computing $\\delta^L$ if the error is the crossentropy loss defined and the output layer is a softmax\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^L = \\nabla_{{z^{\\,L}\\,\\,\\,}}  loss( h(x), e(y) ) = (h(x) - e(y))\n",
    "\\end{equation}\n",
    "\n",
    "### Equation for computing $\\delta^l$ using $\\delta^{l+1}$ for any $1 \\leq l<L$ \n",
    "\n",
    "$$\n",
    "\\delta^l = \\big(W^{l+1 \\,\\,} \\big)^{\\,T}  \\delta^{l+1} .* g'(z^l)\n",
    "$$\n",
    "\n",
    "### Equation for computing the gradient of the weignts at every layer using $\\delta^l$ and $a^{l-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_{W^l} = \\big( a^{l-1\\,\\,} \\big)^{\\,T}  \\delta^l \n",
    "$$\n",
    "\n",
    "### Equation for computing the gradient of the biases at every layer using $\\delta^l$ and $a^{l-1}$\n",
    "$$\n",
    "\\nabla_{b^l} =  \\delta^l \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Hinton matlab code\n",
    "\n",
    "\n",
    "    %%% Error back-propagation\n",
    "    df = [];\n",
    "\n",
    "    Ix = IO;\n",
    "    \n",
    "    %%% do not use outputLayer{nHiddenLayers}: nHiddenLayers may be 0\n",
    "    dw = outputHiddenLayers' * Ix; \n",
    "    df{nHiddenLayers+1} = dw;\n",
    "\n",
    "    for nLayer=nHiddenLayers:-1:1\n",
    "      Ix = (Ix * Weights{nLayer+1}') .* MLE_MultilayerPerceptron_DerivativeFactor...\n",
    "                                           (ActFunHiddens{nLayer},outputLayer{nLayer});\n",
    "      \n",
    "      %%% removes the constant column (the added ones for the bias)\n",
    "      Ix = Ix(:,1:end-1);   \n",
    "      if nLayer > 1\n",
    "        dw = outputLayer{nLayer-1}' * Ix; \n",
    "      else\n",
    "        dw = Data' * Ix;                  \n",
    "      end;\n",
    "      df{nLayer} = dw;\n",
    "    end;\n",
    "    \n",
    "    \n",
    "#### derivatives activations\n",
    "\n",
    "\n",
    "    function DerivativeFactor = MLE_MultilayerPerceptron_DerivativeFactor...\n",
    "                                  (ActFun,outputLayerAct);\n",
    "\n",
    "    if strcmp(ActFun,'tanhyper')\n",
    "      DerivativeFactor = 1 - outputLayerAct .* outputLayerAct;\n",
    "    elseif strcmp(ActFun,'logistic')\n",
    "      DerivativeFactor = outputLayerAct .* (1 - outputLayerAct);\n",
    "    elseif strcmp(ActFun,'hardtanhyper')\n",
    "      DerivativeFactor = ones(size(outputLayerAct));     %%% set to 0 if outputLayerAct<-1 or outputLayerAct>+1\n",
    "      DerivativeFactor = DerivativeFactor .* (outputLayerAct > -1) .* (outputLayerAct < +1);\n",
    "    elseif strcmp(ActFun,'reclinear')\n",
    "      DerivativeFactor = (outputLayerAct > 0);\n",
    "    elseif strcmp(ActFun,'softreclinear')                %%% softplus\n",
    "      DerivativeFactor = 1-exp(-outputLayerAct);         %%% y = log(1+e^x) => dy = 1/(1+e^{-x}) = 1-e^{-y}\n",
    "    elseif strcmp(ActFun,'linear')\n",
    "      DerivativeFactor = 1;\n",
    "    elseif strcmp(ActFun,'sine')\n",
    "      DerivativeFactor = +sqrt( 1 - outputLayerAct.^2 ); %%% sine/cosine: we may lose the sign (we would need Data*Weights)???\n",
    "    elseif strcmp(ActFun,'cosine')\n",
    "      DerivativeFactor = -sqrt( 1 - outputLayerAct.^2 ); %%% sine/cosine: we may lose the sign (we would need Data*Weights)???\n",
    "    else error('MLE_MultilayerPerceptron_DerivativeFactor: ActFun not implemented');\n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_classifer = [LinearLayer{Float32}(input_dim, hidden_dim),\n",
    "                 ReluActivation{Float32}(hidden_dim),\n",
    "                 SoftMaxLayer{Float32}(hidden_dim, output_dim)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delta (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function delta(softmax_output::SoftMaxLayer,\n",
    "               loss::crossentropy_loss,\n",
    "               Xbatch::Array, \n",
    "               Y_enc::Array)\n",
    "\n",
    "    return Xbatch - Y_enc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,25)(500,25)(10,25)"
     ]
    }
   ],
   "source": [
    "aux = forward(mlp_classifer[1], Xbatch)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[2], aux)\n",
    "print(size(aux))\n",
    "aux = forward(mlp_classifer[3], aux)\n",
    "print(size(aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function compute_gradients(mlp, loss, X, Y)\n",
    "    \n",
    "    activations = []\n",
    "    for layer in mlp\n",
    "        push!(activations, forward(layer, X))\n",
    "        \n",
    "    #loss(activations[end], Y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Any,1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " [1,2,3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(a,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " [1,2,3]       \n",
       " [1 2 3; 3 3 3]\n",
       " [1 2 3; 3 3 3]\n",
       " [1 2 3; 3 3 3]\n",
       " [1 2 3; 3 3 3]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(a,[1 2 3;  3 3 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Int64,2}:\n",
       " 1  2  3\n",
       " 3  3  3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Int64,2}:\n",
       " 1  2  3\n",
       " 3  3  3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Int64,2}:\n",
       " 1  2  3\n",
       " 3  3  3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 500, 1000)\n",
    "W2 = rand(T, 500, 500)\n",
    "W3 = rand(T, 10, 500)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W3', error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W2', dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('N', T(1.0), W1', dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN(input, error)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Float32\n",
    "W1 = rand(T, 2048, 512 * 512)\n",
    "W2 = rand(T, 1024, 2048)\n",
    "W3 = rand(T, 10, 1024)\n",
    "dW1, dW2, dW3 = zeros(W1), zeros(W2), zeros(W3)\n",
    "out1, out2, out3 = zeros(T, 2048), zeros(T, 1024), zeros(T, 10)\n",
    "dOut1, dOut2, dOut = zeros(T, 2048), zeros(T, 1024), zeros(T, 512 * 512)\n",
    "\n",
    "function mockNN2(input::Array{Float32, 1}, error::Array{Float32, 1})\n",
    "  # Forward\n",
    "  BLAS.gemv!('N', T(1.0), W1, input, T(0.0), out1)\n",
    "  BLAS.gemv!('N', T(1.0), W2, out1, T(0.0), out2)\n",
    "  BLAS.gemv!('N', T(1.0), W3, out2, T(0.0), out3)\n",
    "\n",
    "  # Backward\n",
    "  # ∂E/∂inputs and ∂E/∂W\n",
    "  fill!(dW3, 0)\n",
    "  fill!(dOut2, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W3, error, T(0.0), dOut2)\n",
    "  BLAS.ger!(T(1.0), error, out2, dW3)\n",
    "  \n",
    "  fill!(dW2, 0)\n",
    "  fill!(dOut1, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W2, dOut2, T(0.0), dOut1)\n",
    "  BLAS.ger!(T(1.0), dOut2, out1, dW2)\n",
    "\n",
    "  fill!(dW1, 0)\n",
    "  fill!(dOut, 0)\n",
    "  BLAS.gemv!('T', T(1.0), W1, dOut1, T(0.0), dOut)\n",
    "  BLAS.ger!(T(1.0), dOut1, input, dW1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = rand(T, 512 * 512)\n",
    "error = rand(T, 10)\n",
    "@time mockNN(input, error)\n",
    "for i in 1:10\n",
    "  input = rand(T, 512 * 512)\n",
    "  error = rand(T, 10)\n",
    "  @time mockNN2(input, error)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
