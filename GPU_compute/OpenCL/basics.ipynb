{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCL basics\n",
    "\n",
    "#### This tutorial is build  and heavily inspired in\n",
    "\n",
    "https://www.youtube.com/watch?v=RKyhHonQMbw\n",
    "\n",
    "\n",
    "There are 2 subjects in an OpenCL program.\n",
    "\n",
    "- **Host**: Piece of hardware that tells devices what to do to the devices. A host can give orders to several devices.\n",
    "\n",
    "\n",
    "- **Device**: Piece of hardware that executes work for the host.\n",
    "\n",
    "\n",
    "### Parts of a device\n",
    "\n",
    "A device is a piece of hardware that is made of **compute units**, **Global Memory** and **Constant Memory**. Each compute unit has several processing elements.\n",
    "\n",
    "- **Compute Unit (CU)**: A compute unit is made of processing elements (PE) and **local memory**.\n",
    "\n",
    "    - **Local memory of a CU**: Memory shared across all processing elements of the CU. It is a very efficient way to share data acrross all PE elements of the CU. This data cannot be accessed by other compute units (that is why it is called local).\n",
    "\n",
    "    - The following diagram shows a compute unit made of 6 processing elements. The compute unit has some local memory accesible to all processing elements. Moreover each processing element has some private memory.\n",
    "\n",
    "```\n",
    "    PE - private mem    PE - private mem\n",
    "    PE - private mem    PE - private mem\n",
    "    PE - private mem    PE - private mem\n",
    "    [ ---------- Local Memory --------- ] \n",
    "``` \n",
    "\n",
    "    \n",
    "- **Processing Element (PE) **: harware piece that executes instructions with a small **private memory**.\n",
    "\n",
    "\n",
    "- **Global Memory**: Main memory of the device. This memory is shared with all processing elements. The host can access this memory. This could be useful for example to copy data from the host memory (usually RAM) to the device memory (for example GDDR5) or vice versa. \n",
    "    - This memory is persistent. If the host puts data in the Global memory and some computations are done, this data will still be there, unless the host explicitly frees the memory.\n",
    "    \n",
    "\n",
    "- **Constant Memory**: Is shared among all processing elements but it is **read-only memory**. It is a very efficient way to share data with all the PE of the device.\n",
    "\n",
    "\n",
    "#### Summary of the types of memory:\n",
    "\n",
    "- Device memory: **Global memory** and **Constant Memory**\n",
    "- CU memory: **Local memory**\n",
    "- PE memory: **private memory**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant sum_kernel\n"
     ]
    }
   ],
   "source": [
    "using OpenCL\n",
    "\n",
    "const sum_kernel = \"\n",
    "   __kernel void sum(__global const float *a,\n",
    "                     __global const float *b,\n",
    "                     __global float *c)\n",
    "    {\n",
    "      int gid = get_global_id(0);\n",
    "      c[gid] = a[gid] + b[gid];\n",
    "    }\n",
    "\"\n",
    "a = rand(Float32, 50_000)\n",
    "b = rand(Float32, 50_000)\n",
    "\n",
    "device, ctx, queue = cl.create_compute_context()\n",
    "\n",
    "a_buff = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=a)\n",
    "b_buff = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=b)\n",
    "c_buff = cl.Buffer(Float32, ctx, :w, length(a))\n",
    "\n",
    "p = cl.build!(cl.Program(ctx, source=sum_kernel))\n",
    "k = cl.Kernel(p, \"sum\")\n",
    "\n",
    "queue(k, size(a), nothing, a_buff, b_buff, c_buff)\n",
    "\n",
    "r = cl.read(queue, c_buff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using OpenCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{OpenCL.cl.Device,1}:\n",
       " OpenCL.Device(Intel(R) Core(TM) i7-3720QM CPU @ 2.60GHz on Apple @0x00000000ffffffff)\n",
       " OpenCL.Device(HD Graphics 4000 on Apple @0x0000000001024400)                         \n",
       " OpenCL.Device(GeForce GT 650M on Apple @0x0000000001022700)                          "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCL Host API\n",
    "\n",
    "The main parts of the Host API are:\n",
    "\n",
    "- **platform**: is an implementation of OpenCL. Platforms are drivers for the devices which expose devices to the host.\n",
    "\n",
    "    - In this example we have a platform for the nvidia GPU and a platform for the intel HD4000 and a platform for the intel CPU. The platform discovers devices available to the host.\n",
    "\n",
    "\n",
    "- **context**: Is a container with devices and memory.\n",
    "\n",
    "    - You create a context for a specific platform\n",
    "    - You cannot have multiple platforms in a context.\n",
    "    - Most operations are related to a context. Explicitly or implicitly.\n",
    "\n",
    "\n",
    "- **program**: Programs are just collections of kernels.\n",
    "     - You must extract kernels from you program to call them\n",
    "     - OpenCL applications have to load kernels.\n",
    "     - Kernels have to be complied. They are OpenCL C source code.\n",
    "     - Kernels can be loaded from a binary representation.\n",
    "     - Programs are device specific.\n",
    "    \n",
    "    \n",
    "#### Asynchronous  calls\n",
    "\n",
    "The host manages devices asynchronously for best performance. The device management can be summarized as follows:\n",
    "\n",
    "- Host issues commands to the device.\n",
    "- Commands tell the device to do something.\n",
    "- Devices take commands and do what is programmed.\n",
    "- The host waits for commands to complete.\n",
    "- Commands can be dependent on other commands.\n",
    "- OpenCL commands are issued by **`clEnqueue`** calls.\n",
    "    - A **`cl_event`** returned by **`clEnqueue*`** calls is used for dependencies.\n",
    "\n",
    "\n",
    "#### Commands and command-queues\n",
    "\n",
    "\n",
    "You can think as the Host to be the boss and devices are just people working for the boss. The command queue allows the boss to talk to each of the persons (devices). The boss can put work to the people.\n",
    "\n",
    "OpenCL has commad-queues which allow the host to pass work to the devices. \n",
    "\n",
    "- A command-queue is a attached to a single device.\n",
    "- There can be as many command-queues as you want.\n",
    "- **`clEnqueue*`** commands have a command-queue parameter.\n",
    "\n",
    "```\n",
    "Host -->  [ command-queue  ]  --> Device\n",
    "```\n",
    "\n",
    "Let us assume we have several `cl_event` tasks.\n",
    "\n",
    "```\n",
    "Host -->  [ e1 e2 e3 e4 e5  ]  --> Device\n",
    "```\n",
    "\n",
    "The Device will pick one at a time and execute it. The host can find out that a command has been completed by the device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.Context(@0x00007fa130665e90 on GeForce GT 650M)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = cl.Context(cl.devices()[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.CmdQueue(@0x00007fa13050e640)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue = cl.CmdQueue(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srand(123)\n",
    "x = rand(Float32, 100000);\n",
    "y = rand(Float32, 100000);\n",
    "o = zeros(Float32,100000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing functions on a device\n",
    "\n",
    "\n",
    "OpenCL executes kernel functions on the device.\n",
    "\n",
    "yeah but... What is a kernel? kernels are functions written in C with some syntax sugar.\n",
    "\n",
    "#### Kernel calls\n",
    "\n",
    "Kernels calls contain 2 main parts: (argument_list, execution_parameters)\n",
    "\n",
    "- Like most functions they have a function argument list.\n",
    "- They have **external execution parameters** that control parallelism.\n",
    "\n",
    "### Host and Device rols\n",
    "\n",
    "The Host coordinates the execution of kernel calls. Nevertheless kernels are executed on the device. \n",
    "\n",
    "The Host has to \n",
    "\n",
    "- Provide arguments to the kernel.\n",
    "- Provide execution parameters (paralelism control) to launch the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What the host does\n",
    "\n",
    "Paralelism is created by invoking the same kernel function many times. The argument list of a kernel is  identical for all invocations. Notice that invocation is different than kernel call.\n",
    "\n",
    "You run the kernel once and then the kernel is invoked many times with the same argument list for all invocations.\n",
    "\n",
    "The strategy for opencl is to invoke the same function over and over. The amount of times is invoked is given by the execution parameters of the kernel call.\n",
    "\n",
    "The host needs to set extra execution parameters prior to launch a kernel. \n",
    "\n",
    "### Index Space: NDRange\n",
    "\n",
    "The paradigm of parallel computation in OpenCL is designed around having the same operations on different data. We have a single kernel that is runned may times on different data slices.\n",
    "\n",
    "How do kernels know in what data they should be working? If the argument list is identicall for all invocations of the kernel this seems imposible to do.\n",
    "\n",
    "A kernel knows what data has to work on because ** execution parameters provide an index space** and **each function invocation can access its index**. \n",
    "The index space is n-Dimensional.\n",
    "\n",
    "\n",
    "\n",
    "#### Loop example\n",
    "\n",
    "Let us consider we want to compute `func(a,b)` 10 times. A standard way could be using a for loop:\n",
    "\n",
    "```C\n",
    "for (size_t global_id=0; global_id < global_work_size, ++global_id)\n",
    "{\n",
    "    func(a,b);\n",
    "}\n",
    "```\n",
    "\n",
    "In opencl there is a particular terminology to refer to the different elements \"in a for loop\". Notice that we used\n",
    "\n",
    "- **`global_id`** to the index traking the iteration.\n",
    "\n",
    "- **`global_work_size`** to the total number of iterations.\n",
    "\n",
    "If we have a situation where there is an offset to the for loop we call the offset **\"global work offset\"**.\n",
    "\n",
    "```C\n",
    "for (size_t global_id = global_work_offset; global_id < global_work_size +  global_work_offset, ++global_id)\n",
    "{\n",
    "    func(a,b);\n",
    "}\n",
    "```\n",
    "\n",
    "- **`work dimension`** corresponds to the number of for loops that control kernel invocations.\n",
    "\n",
    "In the wollowing example the `work dimension` would be 3.\n",
    "```C\n",
    "for (size_t i =0, i < size_i; ++i)\n",
    "    for (size_t j =0, j < size_j; ++j)\n",
    "        for (size_t k=0, k < size_k; ++k)\n",
    "            func(a,b);\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NDrange as index Space\n",
    "\n",
    "- Do not think of for loops since they are inherently sequential.\n",
    "- Think of a set of indicies where each element in the set is a tuple of dimension ND.\n",
    "- Each invocation of a kernel pulls a random index form the set of indices.\n",
    "- The index Space is populated before kernel execution.\n",
    "- An invoked kernel picks an index form the set and runs.\n",
    "- The kernel call stops when the index Space is empty.\n",
    "\n",
    "#### Definitions NDRange\n",
    "\n",
    "- **Work-item** is an invocation of a kernel for a particular index.\n",
    "\n",
    "\n",
    "- **Global ID**: Globally unique id for a work-item (from the index space).\n",
    "\n",
    "\n",
    "- **Global work size**: Number of work-items (per dimension).\n",
    "\n",
    "\n",
    "- ** Work dimension**: Dimension of the index space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_cl = \"\n",
    "#pragma OPENCL EXTENSION cl_khr_byte_addressable_store : enable\n",
    "__kernel void cl_sum(__global float32 *x,\n",
    "                     __global float32 *y, \n",
    "                     __global float32 *o)\n",
    "{\n",
    "    int gid = get_global_id(0);\n",
    "    o[gid] = x[gid] + y[gid];\n",
    "}\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution model \n",
    "\n",
    "When we want to invoke a kernel many times we build an index pace. From the index space a kernel picks an index invokes itself with that index and removes the index from the index space. \n",
    "\n",
    "Remember that all invocations have the same argument list.\n",
    "\n",
    "\n",
    "- **Work-item** is an invocation of a kernel for a particular index.\n",
    "\n",
    "- **Global ID**: Globally unique id for a work-item (from the index space).\n",
    "\n",
    "- **Global work size**: Number of work-items (per dimension).\n",
    "\n",
    "- ** Work dimension**: Dimension of the index space.\n",
    "\n",
    "\n",
    "\n",
    "### Device model\n",
    "\n",
    "We have talked about devices beeing a collection of compute units with global and constant memory. Where each compute unit is a collection of procesing elements sharing local memory.\n",
    "\n",
    "### Execution model and Device model\n",
    "\n",
    "Since processing elements in the device run instructions we should run our code on them. Which means that **work-items should work on processing elements**.\n",
    "\n",
    "Since work-items are sinmply invocations of the kernel we want to assign multiple work items to each processing element. Notice that we want this since we have to think about how to handle the case of having a bigger global work size than the number of processing elements.\n",
    "\n",
    "#### Work-groups and work-items\n",
    "\n",
    "kernel invocations are done by processing elements. Processing elements are placed into compute units each of which contains local memory. Since kernel invocations probably need some data we want to use the **local memory** in every compute unit insted of the global memory. To do so we will partition the global work into smaller pieces.\n",
    "\n",
    "Each partition of the global work is called a work-group. Notice that:\n",
    "\n",
    "- Compute Unit (CU) is a collection of Processing elements (PE).\n",
    "- A work-group (WG) is a collection of Work-items (WI).\n",
    "\n",
    "Therefore there is a natural correspondence between copute units and work-groups.\n",
    "\n",
    "We will execute work-groups in compute units.\n",
    "- Compute unit local memory is shared by the work-group. That means that all work-items in the work group share local memory.\n",
    "- Work-items in the work-group are mapped to processing elements in the Compute unit.\n",
    "\n",
    "Since the number of processing elements in a compute unit is \"device specific\" we want the work-group size to match the number of processing elements in a compute unit.\n",
    "\n",
    "\n",
    "#### Work item world\n",
    "\n",
    "A work-item can access different types of memory:\n",
    "\n",
    "- Private memory from the work group\n",
    "- Constant memory\n",
    "- Global memory\n",
    "\n",
    "A work-item can know:\n",
    "\n",
    "- The work-group id\n",
    "- The size fo work-groups\n",
    "- The global id\n",
    "- The global work size\n",
    "\n",
    "#### Work-group size\n",
    "\n",
    "The maximum work-group size even though it its defined by software is a device characteristic. You can query the device to determine this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.CmdQueue(@0x00007fa1301af2f0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue = cl.CmdQueue(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buffer{Float32}(@0x00007fa130689170)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_buff = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=x)\n",
    "y_buff = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=y)\n",
    "o_buff = cl.Buffer(Float32, ctx, (:r, :copy), hostbuf=o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.Program(@0x00007fa13057d660)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prg = cl.Program(ctx, source=sum_cl) |> cl.build!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.Kernel(\"cl_sum\" nargs=3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = cl.Kernel(prg, \"cl_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenCL.Event(@0x00007fa130651fa0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue(k, length(x), nothing, x_buff, y_buff, o_buff)\n",
    "#cl.copy!(queue, out, o_buff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = cl.read(queue, c_buff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mSuccess!\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "if isapprox(norm(r - (a+b)), zero(Float32))\n",
    "    info(\"Success!\")\n",
    "else\n",
    "    error(\"Norm should be 0.0f\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-dimensional work-groups\n",
    "\n",
    "\n",
    "Work groups can have multiple dimensions.\n",
    "This can be interpreted as\n",
    "\n",
    "- Geometrically\n",
    "- Pulling n-dimensional tuples from  a set.\n",
    "\n",
    "The device maximum work-group size is a single integer. For example, 32. Nevertheless Work-groups can be n-dimensional. For example the maximum work-group size could be 32 but work-groups could be launched with (8,2,1) dimensions. \n",
    "\n",
    "It is important to know that for the device the work-group size is 1 dimensional. A multidimensional work group size is simply an abstraction for the programmer.\n",
    "\n",
    "For example we could have some code where the work-group size is  `(w1,w2,...,wk)`. As long as `w1*w2*...*wk<=max` where `max` is the maximum work-group size the code will run fine. If `w1*w2*...*wk>max` then the host API will return an error.\n",
    "\n",
    "\n",
    "#### Vector example |work-groups| <= |CU|\n",
    "\n",
    "Assume `global_work_size=32` and `work_group_size=8`. Given a vector of length 32 OpenCL would automatically\n",
    "\n",
    "- Make 4  work-groups containing 8 work-items. \n",
    "- Each work group would given to a compute unit.\n",
    "- Each work group will give a work item to a processing element automatically.\n",
    "\n",
    "\n",
    "#### Vector example |work-groups| > |CU|\n",
    "\n",
    "Assume `global_work_size=32` and `work_group_size=8`. Given a vector of length 32 OpenCL and assuming for example we have a single compute unit, openCL will invoke one work group at a time until the 4 work-groups have been executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel calls\n",
    "\n",
    "Host must provide execution dimensions to the device. This will create an index space.\n",
    "\n",
    "Remember that Global memory is persistant between kernel invocations but Constant, Local and Private memory is just scratch space as is reset per kernel call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
